{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e6f90b9d7f671e9cfce5e0d2565311a3a9681f9259769709821c7c40a297c966"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Online Reviews Analysis - (Part-2)\n",
    "This article is an extention to [Online Reviews Analysis - (Part-1)](https://www.linkedin.com/pulse/online-reviews-analysis-part-1-anas-buhayh/?trackingId=CkpqzrXBTMWsvpjKb3ZW5A%3D%3D). The first part of this series explained some techniques that can be used to summerize reviews and measure feature sentiment. This article would be relatively shorter to reduce redudancy. \n",
    "\n",
    "In this article, we will implement the machine learning library XGBoost to check if there is a relationship between the customers rating and the adjectives they used in their review. unlike the first article, this one will be mostly code and it can also serve as an reference for natural language processing; understanding of machine learning is required to get the most of the material."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing libraries and data\n",
    "\n",
    "we will start with importing the libraries and the data. Also, we need to make sure to drop the empty reviews"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('product_reviews.csv').drop(columns=['Product'], axis=1)\n",
    "df = df[df['Review_Text'] != 'none']\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "source": [
    "## Creating corpus\n",
    "\n",
    "Creating the corpus in this example is not any different from the previous one. The only difference is that in this example we need to only keep the adjectives in the reviews."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\bhiha\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['Review_Text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    review = [word for word in review if not word in set(all_stopwords)]\n",
    "    # Keeping only adjectives\n",
    "    tags = nltk.pos_tag(review)\n",
    "    review = [word for word, pos in tags if (pos == 'JJ' or pos == 'JJR' or pos == 'JJS')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "source": [
    "# Training the model\n",
    "\n",
    "We need to turn the features/adjectives into vectors in order to train the model. Machine learning models cannot work with text in it's raw form. in simple words, feature [vectorizing](https://towardsdatascience.com/different-techniques-to-represent-words-as-vectors-word-embeddings-3e4b9ab7ceb4) means converting the words to vectors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "source": [
    "Now we split the data into training and test data. where 80% of the data will be used for training and the other 20% is used for testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n"
   ]
  },
  {
   "source": [
    "it's time to train the model. I used [XGBoost](https://xgboost.readthedocs.io/en/latest/) classifier to predict the customer rating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[17:23:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "let's check the accuracy of the results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 31   1   2   3  32]\n [  4   0   2   0  14]\n [  1   1   1   4  16]\n [  1   0   0   2  46]\n [  1   0   2   5 314]]\n0.7204968944099379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "acs = accuracy_score(y_test, y_pred)\n",
    "print(acs)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}